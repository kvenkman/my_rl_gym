{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# Import ML libraries\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, qnetwork_file=None, batch_size = 32, train = True):\n",
    "        self.env = env\n",
    "        self.epsilon = 1.0 if (train) else 0.\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.9995 if (train) else 1.\n",
    "        self.alpha = 0.05\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        self.train = train # Easy way switch train/test\n",
    "\n",
    "        # Qnetwork and target network\n",
    "        self.qnetwork = self.defineNetwork() if (qnetwork_file == None) else load_model(qnetwork_file)\n",
    "        self.target = self.defineNetwork()\n",
    "        self.alignTarget()\n",
    "\n",
    "    def defineNetwork(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(16, input_dim=np.array(1), activation=\"linear\"))\n",
    "        #model.add(Reshape((16,)))\n",
    "        model.add(Dense(16, activation=\"relu\"))  # hidden layer\n",
    "        model.add(Dense(16, activation=\"relu\")) # hidden layer\n",
    "        model.add(Dense(self.env.action_space.n, activation='linear')) # output layer\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.alpha))\n",
    "        return model\n",
    "\n",
    "    def alignTarget(self):\n",
    "        self.target.set_weights(self.qnetwork.get_weights())\n",
    "\n",
    "    def setSimParameters(self, episodes=100, ntimesteps=200):\n",
    "        self.episodes = episodes if (self.train) else 10\n",
    "        self.ntimesteps = ntimesteps\n",
    "\n",
    "    def selectAction(self, state):\n",
    "        if(random.uniform(0, 1) < self.epsilon):\n",
    "            return env.action_space.sample()\n",
    "        return np.argmax(self.qnetwork.predict(np.array(state)))\n",
    "\n",
    "    def addToMemory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def optimize(self): # , state, action, reward, next_state, done\n",
    "        if (self.train and (len(self.memory) > self.batch_size)):\n",
    "            batch = np.array(random.sample(self.memory, self.batch_size))\n",
    "\n",
    "            train_states = np.zeros((self.batch_size, 1))\n",
    "            target_rewards = np.zeros((self.batch_size, self.env.action_space.n))\n",
    "\n",
    "            for i, (state, action, reward, next_state, done) in enumerate(batch):\n",
    "                target = self.qnetwork.predict(state)\n",
    "                if (done):\n",
    "                    target[0][action] = reward\n",
    "                else:\n",
    "                    t = self.target.predict(next_state)\n",
    "                    target[0][action] = reward + self.gamma*np.amax(t)\n",
    "\n",
    "                train_states[i] = state\n",
    "                target_rewards[i, :] = target\n",
    "\n",
    "            self.qnetwork.fit(np.squeeze(train_states), np.squeeze(target_rewards), epochs = 1, verbose = 0)\n",
    "\n",
    "    def updateEpsilon(self):\n",
    "        self.epsilon = np.maximum(self.epsilon_decay*self.epsilon, self.epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 102       \n",
      "=================================================================\n",
      "Total params: 678\n",
      "Trainable params: 678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Define agent, sim parameters\n",
    "taxiAgent = Agent(env, qnetwork_file=\"taxi_qnetwork_5120.h5\", batch_size = 64, train = False)\n",
    "taxiAgent.setSimParameters(episodes = 10000, ntimesteps = 200)\n",
    "\n",
    "taxiAgent.qnetwork.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "Current episode, timestep, state, action taken:  6 77 483 0\n"
     ]
    }
   ],
   "source": [
    "# Test sim run:\n",
    "# taxiAgent.epsilon=0.5\n",
    "for j in range(10): # 10 episodes\n",
    "    state = env.reset()\n",
    "    for i in range(200):\n",
    "        action = taxiAgent.selectAction(np.array(state).reshape(1)) # env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        taxiAgent.addToMemory(np.array(state).reshape(1), action, reward, \\\n",
    "                                    np.array(next_state).reshape(1), done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        print(\"Current episode, timestep, state, action taken: \", j, i, state, action)\n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)    \n",
    "\n",
    "        if(done):\n",
    "            print(\"success!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "Current episode number, timesteps, epsilon:  9 200 199 0.05\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "save_point = 10\n",
    "current_reward = [0]\n",
    "current_episode = [0]\n",
    "\n",
    "for i in range(taxiAgent.episodes): # taxiAgent.episodes\n",
    "    state = env.reset()\n",
    "    clear_output(wait=True) # Works only inside jupyter notebooks\n",
    "    print(\"Episode number: {} of {}. Current epsilon: {}\".format(str(i),\n",
    "                                                                 str(taxiAgent.episodes), str(taxiAgent.epsilon)))\n",
    "\n",
    "    for j in range(taxiAgent.ntimesteps): # taxiAgent.ntimesteps\n",
    "        if not taxiAgent.train:\n",
    "            env.render()\n",
    "            print(\"Current episode number, timesteps, epsilon: \", i, taxiAgent.ntimesteps, j, taxiAgent.epsilon)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        action = taxiAgent.selectAction(np.array(state).reshape(1))\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        taxiAgent.addToMemory(np.array(state).reshape(1), action, reward, np.array(new_state).reshape(1), done)\n",
    "        taxiAgent.optimize()\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if (done):\n",
    "            break\n",
    "\n",
    "    taxiAgent.updateEpsilon()\n",
    "    taxiAgent.alignTarget()\n",
    "\n",
    "    if (taxiAgent.train):\n",
    "        if(i == save_point):\n",
    "            taxiAgent.qnetwork.save(\"taxi_qnetwork_\"+str(i)+\".h5\")\n",
    "            os.system(\"git add taxi_*.h5\")\n",
    "            os.system(\"git commit -m \\\"autoupdate saved taxi agent network \\\"\")\n",
    "            save_point *= 2\n",
    "            current_reward.append(reward)\n",
    "            current_episode.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxiAgent.qnetwork.save(\"taxi_qnetwork_\"+str(i)+\".h5\")\n",
    "os.system(\"git add taxi_qnetwork_*.h5\")\n",
    "os.system(\"git commit -m \\\"autoupdate saved model\\\"\")\n",
    "os.system(\"git push origin master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(current_episode, current_reward)\n",
    "plt.title(\"Rewards vs Episode #\", size=15)\n",
    "plt.xlabel(\"Episode #\", size=12)\n",
    "plt.ylabel(\"Rewards\", size=12)\n",
    "plt.grid()\n",
    "plt.savefig(\"taxi_rewards.png\")\n",
    "os.system(\"git add taxi_rewards.png\")\n",
    "os.system(\"git commit -m \\\"Taxi rewards plot\\\"\")\n",
    "os.system(\"git push origin master\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
